p8105\_HW3\_acn2138
================

Amanda Nagle’s HW 3

# Problem 1

Importing and cleaning the instacart dataset.

``` r
library(tidyverse)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%")

library(p8105.datasets)
data("instacart")
```

The data in the instacart dataset has one row per item in customer
order, with customer and order variables. It has 1384617 and 15 columns.

``` r
items_from_aisle = instacart %>%
                   count(aisle) %>%
                    arrange(desc(n))

items_from_aisle %>%
 filter(n > 10000) %>%
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>%
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 270, vjust = 0.5, hjust = 1))
```

<img src="p8105_HW2_acn2138_files/figure-gfm/unnamed-chunk-2-1.png" width="90%" />
Making a table:

``` r
instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
   group_by(aisle) %>%
  count(product_name) %>%
  mutate(rank = min_rank(desc(n))) %>%
  filter(rank < 4) %>%
  arrange(aisle, rank) %>%
  knitr::kable()
```

| aisle                      | product\_name                                 |    n | rank |
| :------------------------- | :-------------------------------------------- | ---: | ---: |
| baking ingredients         | Light Brown Sugar                             |  499 |    1 |
| baking ingredients         | Pure Baking Soda                              |  387 |    2 |
| baking ingredients         | Cane Sugar                                    |  336 |    3 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |   30 |    1 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |   28 |    2 |
| dog food care              | Small Dog Biscuits                            |   26 |    3 |
| packaged vegetables fruits | Organic Baby Spinach                          | 9784 |    1 |
| packaged vegetables fruits | Organic Raspberries                           | 5546 |    2 |
| packaged vegetables fruits | Organic Blueberries                           | 4966 |    3 |

Apples v Icecream

``` r
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```

    ## `summarise()` regrouping output by 'product_name' (override with `.groups` argument)

    ## # A tibble: 2 x 8
    ## # Groups:   product_name [2]
    ##   product_name       `0`   `1`   `2`   `3`   `4`   `5`   `6`
    ##   <chr>            <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
    ## 1 Coffee Ice Cream  13.8  14.3  15.4  15.3  15.2  12.3  13.8
    ## 2 Pink Lady Apples  13.4  11.4  11.7  14.2  11.6  12.8  11.9

# Problem 2

Importing and tidying the accelerometer data.

This dataset contains one row for ever entrance/exit in the NYC subway
system. The important variables are the line name and station name
because these allow the user to understand subway stations. The routes
are important to understand which trains stop at the station.

``` r
accelerometer_data = read_csv(file = "./data/accel_data.csv") %>%
                    janitor::clean_names()  %>%
                    pivot_longer(activity_1:activity_1440,
                                 names_to = "minute",
                                 names_prefix = "activity.",
                                 values_to = "activity") %>%
                    mutate(weekday = ifelse(day %in% c("Saturday", "Sunday"), "weekend", "weekday"),
                           day = factor(day,
                                        levels= c("Sunday", "Monday", 
    "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")),
                            day_id = as.factor(day_id),
                            minute = as.numeric(minute)) %>%
                    arrange(day)
```

The accelerometer data is now tidy with 50400, one for each minute of
each day for 35 days. There are 6 columns, one for the week, the numeric
day starting at the first day, the day of the week the minute of the
day, the amount of activity, and if it is a weekend or a weekday.

Creating table of day totals:

``` r
one_day_row = accelerometer_data %>%
  group_by(week,day_id, day, weekday) %>%
  summarize(total_activity= sum(activity)) %>%
  arrange(week, day)
  
knitr::kable(one_day_row)
```

| week | day\_id | day       | weekday | total\_activity |
| ---: | :------ | :-------- | :------ | --------------: |
|    1 | 4       | Sunday    | weekend |       631105.00 |
|    1 | 2       | Monday    | weekday |        78828.07 |
|    1 | 6       | Tuesday   | weekday |       307094.24 |
|    1 | 7       | Wednesday | weekday |       340115.01 |
|    1 | 5       | Thursday  | weekday |       355923.64 |
|    1 | 1       | Friday    | weekday |       480542.62 |
|    1 | 3       | Saturday  | weekend |       376254.00 |
|    2 | 11      | Sunday    | weekend |       422018.00 |
|    2 | 9       | Monday    | weekday |       295431.00 |
|    2 | 13      | Tuesday   | weekday |       423245.00 |
|    2 | 14      | Wednesday | weekday |       440962.00 |
|    2 | 12      | Thursday  | weekday |       474048.00 |
|    2 | 8       | Friday    | weekday |       568839.00 |
|    2 | 10      | Saturday  | weekend |       607175.00 |
|    3 | 18      | Sunday    | weekend |       467052.00 |
|    3 | 16      | Monday    | weekday |       685910.00 |
|    3 | 20      | Tuesday   | weekday |       381507.00 |
|    3 | 21      | Wednesday | weekday |       468869.00 |
|    3 | 19      | Thursday  | weekday |       371230.00 |
|    3 | 15      | Friday    | weekday |       467420.00 |
|    3 | 17      | Saturday  | weekend |       382928.00 |
|    4 | 25      | Sunday    | weekend |       260617.00 |
|    4 | 23      | Monday    | weekday |       409450.00 |
|    4 | 27      | Tuesday   | weekday |       319568.00 |
|    4 | 28      | Wednesday | weekday |       434460.00 |
|    4 | 26      | Thursday  | weekday |       340291.00 |
|    4 | 22      | Friday    | weekday |       154049.00 |
|    4 | 24      | Saturday  | weekend |         1440.00 |
|    5 | 32      | Sunday    | weekend |       138421.00 |
|    5 | 30      | Monday    | weekday |       389080.00 |
|    5 | 34      | Tuesday   | weekday |       367824.00 |
|    5 | 35      | Wednesday | weekday |       445366.00 |
|    5 | 33      | Thursday  | weekday |       549658.00 |
|    5 | 29      | Friday    | weekday |       620860.00 |
|    5 | 31      | Saturday  | weekend |         1440.00 |

``` r
one_day_row %>% 
  group_by(day, weekday) %>%
  summarize(mean(total_activity))
```

    ## # A tibble: 7 x 3
    ## # Groups:   day [7]
    ##   day       weekday `mean(total_activity)`
    ##   <fct>     <chr>                    <dbl>
    ## 1 Sunday    weekend                383843.
    ## 2 Monday    weekday                371740.
    ## 3 Tuesday   weekday                359848.
    ## 4 Wednesday weekday                425954.
    ## 5 Thursday  weekday                418230.
    ## 6 Friday    weekday                458342.
    ## 7 Saturday  weekend                273847.

Can’t really identify any trends from the above table.

Making visualization for day of week

``` r
accelerometer_data %>%
  group_by(day_id) %>%
  ggplot(aes(x = minute, y = activity, group = factor(day_id), color= day)) +
  geom_point(alpha=.5) +
  geom_line()
```

<img src="p8105_HW2_acn2138_files/figure-gfm/unnamed-chunk-7-1.png" width="90%" />

``` r
accelerometer_data %>%
  ggplot(aes(x = minute, y = activity, color= day)) +
  geom_point(alpha=.2)
```

<img src="p8105_HW2_acn2138_files/figure-gfm/unnamed-chunk-7-2.png" width="90%" />
I included geom\_line and group = day\_id in the above code because I
think the question is asking us to show the day groupings and day of
week groupings overtime. Because the time granuals are so small, the
line graph is not nice to look at and I prefer the second graph with
only points.

From the second grapg, we can see that this man does not much activity
before 8:00, with a few exceptions on Thursdays. He is also most active
on any day just after 8 pm.

# Problem 3

Importing and cleaning the NY NOAA data.

``` r
library(p8105.datasets)
data("ny_noaa") 
            
summarize(ny_noaa, mean(tmax, na.rm=true))
```

    ## # A tibble: 1 x 1
    ##   `mean(tmax, na.rm = true)`
    ##                        <dbl>
    ## 1                         NA

``` r
cleaned_noaa_data = ny_noaa %>%
  separate(date, c("year", "month", "day"), sep = "-") %>% 
  mutate(prcp = prcp*10, tmax = as.numeric(tmax), tmin = as.numeric(tmin))

#summarize(cleaned_noaa_data, mean(tmax, na.rm = TRUE))
  
  #mode snowfall 
ny_noaa %>%
  separate(date, c("year", "month", "day"), sep = "-") %>% 
  mutate(prcp = prcp*10, chr_snw = as.character(snow)) %>%
  count(chr_snw) %>%
  arrange(desc(n))
```

    ## # A tibble: 282 x 2
    ##    chr_snw       n
    ##    <chr>     <int>
    ##  1 0       2008508
    ##  2 <NA>     381221
    ##  3 25        31022
    ##  4 13        23095
    ##  5 51        18274
    ##  6 76        10173
    ##  7 8          9962
    ##  8 5          9748
    ##  9 38         9197
    ## 10 3          8790
    ## # ... with 272 more rows

``` r
  #hex plot? box or violin?
```

The New York NOAA data is a subset of the National Oceanic and
Atmospheric Administration’s database of data from weather station. It
has2595176 rows of data. The columns are id, date, prcp, snow, snwd,
tmax, tmin, where prcp is precipitation and snwd is snow depth. however
tmax and tmin are NA or missing for the entire dataset.

Snowfall and snow depth are measured in mm. Precipitation was recorded
in 10ths of mms, but was edited to me mm to be consistent with the other
two measures.

The most common snowfall measure was 0 inches and second was NA. 0
snowfall makes sense as it only snows in New York in the Winter. The NA
values requires further investigations. These NA days should perhaps be
coded as 0s
